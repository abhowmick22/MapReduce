Functionality of the distributed processing framework
-----------------------------------------------------

1. JobTracker does the following tasks
	- Schedule jobs on the slave nodes
	- Run a monitor daemon to check the jobs on the slave nodes
	- re-execute jobs on the slave jobs in the event of failures
	
2. TaskTracker needs to run an RMI providing following services
	-
	
3. Task
	- assume we only get the dfs file path, and invoke a method at the master to 
	  get the local file system path
	- also, we run map/reduce tasks as services bound to a registry (this is the pool)
	
4. Implement a way for jobTracker to receive jobs from clientAPI

5. Assume jobDispatcher listens for messages from slaves on port 10000 and sends messages to slaves on 10001
   JTMonitor waits for health reports on port 10002.
   Also,from clientAPI on port 20000, and also sends messages to clientAPI on port 20001
   
6. Tasktracker need not be highly efficient in accepting job requests, since it can only
   take decisions on whether to accept jobs based on previous outcomes. So it will be
   processing requests sequentially
   
7. We would like to allow different jobtrackers to be able to use different scheduleres,
   hence scheduler is not a static object
   
8. Assumption is that maps finish before reduce tasks

9. Files are always refered to by a name, which is unique across the system. The mapper/
   reeducer which operates on these files fetch the physical location on the LFS from a
   service on the namenode, before starting to work on them. 
   
10. Rough idea of the SmartScheduler Logic
    i. Maintain a data structure containing workload of each node. Select node with min 
       workload.
   ii. Query the namenode service to get locality information. Specifically, get the list
       of fileBlocks stored on the chosen node.
  iii. Split the fileBlock to get fileName and blockNbr. Use fileName to choose the job from
       mapredJobs, get the TaskList for this job. In this, use blockNbr to look up the 
       appropriate taskId. If taskId is not scheduled, choose this for dispatch.
       Else, repeat iii. with the next fileBlock for the node.
       
11. Rough idea of the SimpleScheduler Logic
    i. Just iterate through mapredJobs in a round robin fashion. For a chosen job, choose the
       first unscheduled task. Get the node name where corresponding fileBlock is located.
       Dispatch to that node. Note that only one task is launched for each job. This ensures
       fairness among jobs.  
       
12. Change all occurrences of '==' to '.equals()'