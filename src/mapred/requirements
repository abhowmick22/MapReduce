Functionality of the distributed processing framework
-----------------------------------------------------

1. JobTracker does the following tasks
	- Schedule jobs on the slave nodes
	- Run a monitor daemon to check the jobs on the slave nodes
	- re-execute jobs on the slave jobs in the event of failures
	
2. TaskTracker does the following tasks
	- Process every incoming message from JobTracker
		- Launch new task in an execution thread OR
		- Stop all tasks belonging to given jobId
	- Send ACK, NACK, task completion messages and make changes to
	  internal tables
	- respond to polling requests from JTMonitor

	
3. Task
	- assume we only get the dfs file name, and invoke a method at the master to 
	  get the local file system path
	- also, we run map/reduce tasks as threads (not more than max threshold)
	
4. Implement a way for jobTracker to receive jobs from clientAPI

5. JobDispatcher listens for accept/reject messages from slaves on port 10000 and sends messages to slaves on 10001
   JTMonitor listens finish messages on port 10003.
   JTPolling polls for health reports on port 10004.
   Task sends finish messages to TTMonitor on port 10002, which forwards them to JTMonitor on 10003.
   Also, receive from clientAPI on port 20000, and sends messages to clientAPI on port 20001
   
6. TaskTracker need not be highly efficient in accepting job requests, since it can only
   take decisions on whether to accept jobs based on previous outcomes. So it will be
   processing requests sequentially. JTDispatcher will be blocking till it receives
   info about task getting accepted/rejected.
   Also, it sends back "reject" messages to JobTracker if it cannot accept any more tasks.
   This shifts the onus of rescheduling "rejected" tasks on the JobTracker, hence TaskTracker
   need not queue up tasks or have any scheduling logic, this simplifies the implementation.
   
7. We would like to allow different jobtrackers to be able to use different schedulers,
   hence scheduler is not a static object
   
8. Assumption is that maps finish before reduce tasks

9. Files are always refered to by a name, which is unique across the system. The mapper/
   reeducer which operates on these files fetch the physical location on the LFS from a
   service on the namenode, before starting to work on them. 
   
10. Rough idea of the SmartScheduler Logic
    i. Maintain a data structure containing workload of each node. Select node with min 
       workload.
   ii. Query the namenode service to get locality information. Specifically, get the list
       of fileBlocks stored on the chosen node.
  iii. Split the fileBlock to get fileName and blockNbr. Use fileName to choose the job from
       mapredJobs, get the TaskList for this job. In this, use blockNbr to look up the 
       appropriate taskId. If taskId is not scheduled, choose this for dispatch.
       Else, repeat iii. with the next fileBlock for the node.
       
11. Rough idea of the SimpleScheduler Logic
    i. Just iterate through mapredJobs in a round robin fashion. For a chosen job, choose the
       first unscheduled task. Get the node name where corresponding fileBlock is located.
       Dispatch to that node. Note that only one task is launched for each job. This ensures
       fairness among jobs.  
       
12. Change all occurrences of '==' to '.equals()'

13. To kill an execution thread, simple reset an "alive" flag. The thread checks
    the flag before every map operation or in case of a reduce, wait for it to finish and
    then exit. ?? Any better way to kill a reduce task ??
    
14. For each file block, maintain a subset of records (called split) on which a mapper
	will work
	
15. The cluster has Java 1.6, so can't use String in switch. Use 'enum' instead

16. We can determine the fileBlockName by checking the input file of its parent job, task id,
	and the split size. The file block size is ideally a parameter determined by the framework.
	
17. Task ids are always strictly positive inside tasksList (both map/reduce)

18. The mapredjobs and runningTasks tables in TaskTracker are updated from TTMonitor when jobs
	are finished. A task sends a completion message to TTMonitor, which makes suitable changes
	to mapredjobs and runninTasks tables of TaskTracker, and then forwards those messages
	to JTMonitor. TTMonitor makes changes to TaskTracker state instead of Task because of security,
	Task may be running malicious code.
	
19. Assume that for now, entries for all jobs and tasks in corresponding tables stay after done.
	Ideally, cleaning should happen, but only when a job is done.

TODO
Figure out why the fuck serversocket in child threaed blocks parent thread