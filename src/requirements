Distributed File System:
1. started from the namenode
2. contains the following:
	- A trie consisting of the file system structure
	- At each node in the trie is a directory; it also stores an array list of
	
Requirements from Admin for DFS:
1. Add datanode names and total number of datanodes in config file
2. 

Requirements from the client for DFS:
1. Client provides file upload path on DFS
2. Client provides DFS input and output paths for accessing files
3. A client can add files only to its own username. The username for a client is the machine name
that he is logged in from. ClientAPI checks whether the path of a file on DFS starts with "/dfs/username/"

TODO:
1. Replicate only on nodes with less data on their disk, or with more processing power currently? Doing
the former right now.


Adding files to DFS:
NO '--' ALLOWED IN USERNAME OR ON DFS PATH 
1. Client "adds" file to DFS by giving its local path to Client API, and the DFS path where the file should be placed.
The client api sends the DFS path to DFS service by calling the "addFileToDFS" method. 
2. The DFS, in response to this, creates the path to the file on the DFS, and for the file, assigns the number of 
blocks according to replication factor. It selects the datanodes according to min load to assign the blocks to.
3. Now, DFS sends back a map of the block names (ending with the number of the block) and the list of datanode names
to which each of these blocks needs to be sent.
4. Client api then creates blocks with the provided names and copies them to the datanodes. 
5. The blocks are stored in LocalBaseDir/ directory on the ghc machines. All blocks are stored in same dir since each has username in the beginning. 
6. Finally, the datanodes notify the DFS about which block(s) is given to each datanode. The DFS updates the final
mapping of nodename to block(s) in its dfsMetadata data structure. This is done in the following manner in the confirmBlockReceipt method:
NOTE: The datanode reads the config file to get the LocalBaseDir. Client API supplies the username

On the datanode's local file system, the blocks of a file will be stored in LocalBaseDir/username/ 
directory. The DFS sends this path to the client api who then sends it to the datanodes. The format
of each block's name is --> username-dfs path after username-filename with txt-i where i is the block number 
Hence, the local block names are such that the DFS path to that file can be derived from each of its block names.

Deleting files from DFS:
1. Client "deletes" file from DFS by giving the DFS path to Client API.
The client api sends the DFS path to DFS service by calling the "deleteFilefromDFS" method. 
2. The DFS, in response to this, deletes the reference to the file from the DfsStruct of its parent directory.
3. Now, DFS sends back a map of the block names (ending with the number of the block) and the list of datanode names
where each of these blocks resides (after removing the nodes which never sent the confirmation of adding the block).
4. Client api then contacts the datanodes to delete the blocks.
5. The blocks are stored in LocalBaseDir/username directory on each datanode. 
NOTE: The datanode reads the config file to get the LocalBaseDir. Client API supplies the username

RMI:
1. I haven't copied the ClientApi and DfsService interfaces to a jar for the client and server to copy them to local
machines because I'm assuming that both the client and server have the entire code base for now. So they can
access these interfaces from their local file system without copying from client. Anyway it's not like ClientApi
or DfsService interfaces are provided the actual client. So I'll use 'em the way I like.
2. Also, except for client's mapreduce class, we don't need to share any classes between client and server. So
I haven't created a local directory for that download to server from client right now. Maybe the client can do
that manually and give us the location of where he placed the .class file in our configFile.